{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEd-mtF0PTrv"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to Moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8c9YBduQCI4"
      },
      "source": [
        "## 1 $n$-gram Language Model\n",
        "**Q1**: Expand the above definition of $ p(\\vec{w})$ using naive estimates of the parameters, such as $  p(w_4 \\mid w_2, w_3) \\stackrel{\\tiny{\\mbox{def}}}{=}  \\frac{C(w_2~w_3~w_4)}{C(w_2~w_3)} $ where \\( C(w_2 w_3 w_4) \\) denotes the count of times the trigram $ w_2 w_3 w_4 $ was observed in a training corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQ_Z1g8QZef"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "$ p(\\vec{w}) = p(w_1) \\times \\frac{C(w_1, w_2)}{C(w_1)} \\times \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)} \\times \\ldots \\times \\frac{C(w_{N-2}, w_{N-1}, w_N)}{C(w_{N-2}, w_{N-1})} $\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmgExbf1QtCH"
      },
      "source": [
        "**Q2**: One could also define a kind of reversed trigram language model $p_{reversed}$ that instead assumed the words were generated in reverse order (from right to left):\n",
        "\\begin{align} p_{reversed}(\\vec{w}) \\stackrel{\\tiny{\\mbox{def}}}{=}&p(w_n) \\cdot p(w_{n-1} \\mid w_n) \\cdot p(w_{n-2} \\mid w_{n-1} w_n) \\cdot p(w_{n-3} \\mid w_{n-2} w_{n-1}) \\\\ &\\cdots p(w_2 \\mid w_3 w_4) \\cdot p(w_1 \\mid w_2 w_3) \\end{align}\n",
        "By manipulating the notation, show that the two models are identical, i.e., $ p(\\vec{w}) = p_{reversed}(\\vec{w}) $ for any $ \\vec{w} $ provided that both models use MLE parameters estimated from the same training data (see Q1 above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm1ZGFIaRPCP"
      },
      "source": [
        "**Write your answer:**\n",
        "\n",
        "To prove that the two models are identical, let's start by expanding the definition of $ p_{\\text{reversed}}(\\vec{w}) $ using the MLE parameters:\n",
        "\n",
        "Given:\n",
        "\n",
        "$\n",
        "p(w_{n-1} \\mid w_n) \\stackrel{\\tiny{\\mbox{def}}}{=} \\frac{C(w_n~w_{n-1})}{C(w_n)}\n",
        "$\n",
        "\n",
        "$\n",
        "p(w_{n-2} \\mid w_{n-1}~w_n) \\stackrel{\\tiny{\\mbox{def}}}{=} \\frac{C(w_n~w_{n-1}~w_{n-2})}{C(w_n~w_{n-1})}\n",
        "$\n",
        "\n",
        "and so on...\n",
        "\n",
        "Expanding the reversed trigram model:\n",
        "\n",
        "$\n",
        "p_{\\text{reversed}}(\\vec{w}) = p(w_n) \\times \\frac{C(w_n, w_{n-1})}{C(w_n)} \\times \\frac{C(w_n, w_{n-1}, w_{n-2})}{C(w_n, w_{n-1})} \\times \\ldots \\times \\frac{C(w_3, w_2, w_1)}{C(w_3, w_2)}\n",
        "$\n",
        "\n",
        "Now, let's look at the regular trigram model:\n",
        "\n",
        "$\n",
        "p(\\vec{w}) = p(w_1) \\times \\frac{C(w_1, w_2)}{C(w_1)} \\times \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)} \\times \\ldots \\times \\frac{C(w_{N-2}, w_{N-1}, w_N)}{C(w_{N-2}, w_{N-1})}\n",
        "$\n",
        "\n",
        "Notice that every term in the reversed model has a corresponding term in the regular model but in reverse order. For example, the term $ \\frac{C(w_n, w_{n-1})}{C(w_n)} $ in the reversed model corresponds to the term $ \\frac{C(w_1, w_2)}{C(w_1)} $ in the regular model. Similarly, the term $ \\frac{C(w_n, w_{n-1}, w_{n-2})}{C(w_n, w_{n-1})} $ in the reversed model corresponds to the term $ \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)} $ in the regular model, and so on.\n",
        "\n",
        "Since both models are estimated from the same training data, the counts $ C(\\cdot) $ are the same in both models. Thus, every term in the product for the reversed model has a matching term in the product for the regular model, and the two products are equal.\n",
        "\n",
        "Therefore, for any sentence $ \\vec{w} $:\n",
        "\n",
        "$\n",
        "p(\\vec{w}) = p_{\\text{reversed}}(\\vec{w})\n",
        "$\n",
        "\n",
        "This shows that the two models are indeed identical when using MLE parameters estimated from the same training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEc5kz4RniG"
      },
      "source": [
        "## 2 $N$-gram Language Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kSwtN79jWgp"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m运行具有“base”的单元格需要ipykernel包。\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9HCVQwqkTc_"
      },
      "source": [
        "### 2.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KhFKCzwkaTn"
      },
      "source": [
        "**Code**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19349"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Load the training data\n",
        "with open(\"./data/lm/train.txt\", \"r\") as train_file:\n",
        "    train_data = train_file.readlines()\n",
        "\n",
        "# Tokenize the training data\n",
        "tokens = []\n",
        "for line in train_data:\n",
        "    tokens.extend(line.strip().split())\n",
        "\n",
        "# Build the vocabulary\n",
        "token_counts = Counter(tokens)\n",
        "\n",
        "# Replacing tokens that occur less than three times with <UNK>\n",
        "tokens = ['<UNK>' if token_counts[token] < 3 else token for token in tokens]\n",
        "\n",
        "# Convert tokens that occur less than three times to <UNK>\n",
        "vocab = {token: count for token, count in token_counts.items() if count >= 3}\n",
        "unknown_tokens = sum(count for token, count in token_counts.items() if count < 3)\n",
        "\n",
        "# Add <UNK> to the vocabulary\n",
        "vocab[\"<UNK>\"] = unknown_tokens\n",
        "\n",
        "# Add start-of-sentence <s> and end-of-sentence </s> tokens\n",
        "vocab[\"<s>\"] = len(train_data)  # Each line is treated as a sentence\n",
        "vocab[\"</s>\"] = len(train_data)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "vocab_size\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQNsUA5kfZe"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "1. **Unigram Model**: The number of parameters is equal to the vocabulary size.\n",
        "2. **Bigram Model**: The number of parameters is the square of the vocabulary size, as it accounts for every possible combination of two consecutive tokens.\n",
        "3. **Trigram Model**: The number of parameters is the cube of the vocabulary size, as it considers every possible combination of three consecutive tokens.\n",
        "\n",
        "After calculation, the number of parameters for the n-gram models are as follows:\n",
        "\n",
        "1. **Unigram Model**: 19,349 parameters\n",
        "2. **Bigram Model**: 374,383,801 parameters\n",
        "3. **Trigram Model**: 7,243,952,165,549 parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJzDNMVikkeX"
      },
      "source": [
        "### 2.2 $N$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xxkcs2HykuR2"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(inf, inf)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compute unigram probabilities\n",
        "total_tokens = sum(vocab.values())\n",
        "unigram_probs = {token: count/total_tokens for token, count in vocab.items()}\n",
        "\n",
        "# Compute bigram probabilities\n",
        "bigram_counts = Counter()\n",
        "for line in train_data:\n",
        "    sentence_tokens = ['<s>'] + line.strip().split() + ['</s>']\n",
        "    for i in range(len(sentence_tokens)-1):\n",
        "        bigram_counts[(sentence_tokens[i], sentence_tokens[i+1])] += 1\n",
        "\n",
        "# Replace tokens not in the vocabulary with <UNK> when computing bigram probabilities\n",
        "bigram_probs = {}\n",
        "for (token1, token2), count in bigram_counts.items():\n",
        "    token1 = token1 if token1 in vocab else '<UNK>'\n",
        "    token2 = token2 if token2 in vocab else '<UNK>'\n",
        "    bigram_probs[(token1, token2)] = count / vocab[token1]\n",
        "\n",
        "# Return the first few entries of the distributions for review\n",
        "list(unigram_probs.items())[:5], list(bigram_probs.items())[:5]\n",
        "\n",
        "import math\n",
        "\n",
        "def compute_perplexity_unigram(data, unigram_probs):\n",
        "    N = 0  # Total number of tokens\n",
        "    product = 1.0\n",
        "    for line in data:\n",
        "        tokens = line.strip().split()\n",
        "        N += len(tokens)\n",
        "        for token in tokens:\n",
        "            token = token if token in unigram_probs else '<UNK>'\n",
        "            product *= (1.0 / unigram_probs[token])\n",
        "    return math.pow(product, 1.0/N)\n",
        "\n",
        "def compute_perplexity_bigram(data, unigram_probs, bigram_probs):\n",
        "    N = 0  # Total number of tokens\n",
        "    product = 1.0\n",
        "    for line in data:\n",
        "        tokens = ['<s>'] + line.strip().split() + ['</s>']\n",
        "        N += len(tokens) - 1\n",
        "        for i in range(len(tokens)-1):\n",
        "            token1 = tokens[i] if tokens[i] in unigram_probs else '<UNK>'\n",
        "            token2 = tokens[i+1] if tokens[i+1] in unigram_probs else '<UNK>'\n",
        "            product *= (1.0 / bigram_probs.get((token1, token2), unigram_probs[token2]))\n",
        "    return math.pow(product, 1.0/N)\n",
        "\n",
        "# Compute perplexity for unigram and bigram models on the training set\n",
        "perplexity_unigram_train = compute_perplexity_unigram(train_data, unigram_probs)\n",
        "perplexity_bigram_train = compute_perplexity_bigram(train_data, unigram_probs, bigram_probs)\n",
        "\n",
        "perplexity_unigram_train, perplexity_bigram_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3o9Nez8kvYm"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "The perplexities for both the unigram and bigram models on the training set are infinite. This suggests that there might be some tokens for which the model assigns a probability of zero, leading to infinite perplexity. This is a common problem when using basic n-gram models without any smoothing techniques, as they assign zero probabilities to unseen n-grams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOQUqM73kzf-"
      },
      "source": [
        "### 2.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LgXRmJwk3Y-"
      },
      "source": [
        "#### 2.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFG7jCIRk7Qw"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1394.0528107520793, 1574.5413718583468)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def get_bigrams(tokens):\n",
        "    \"\"\"Return a list of bigrams from the list of tokens.\"\"\"\n",
        "    return [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
        "\n",
        "def laplace_smoothing_bigram_prob(bigram, bigram_counts, unigram_counts, vocab_size):\n",
        "    \"\"\"Compute the probability of a bigram using Laplace smoothing.\"\"\"\n",
        "    return (bigram_counts[bigram] + 1) / (unigram_counts[bigram[0]] + vocab_size)\n",
        "\n",
        "def compute_perplexity_laplace(tokens, bigram_counts, unigram_counts, vocab_size):\n",
        "    \"\"\"Compute the perplexity of a list of tokens using Laplace smoothed bigram probabilities.\"\"\"\n",
        "    N = len(tokens)\n",
        "    log_prob_sum = 0\n",
        "    for i in range(N - 1):\n",
        "        bigram = (tokens[i], tokens[i + 1])\n",
        "        log_prob_sum += math.log2(laplace_smoothing_bigram_prob(bigram, bigram_counts, unigram_counts, vocab_size))\n",
        "    return math.pow(2, -log_prob_sum / N)\n",
        "# Computing bigram counts\n",
        "bigrams = get_bigrams(tokens)\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Computing unigram counts\n",
        "unigram_counts = Counter(tokens)\n",
        "\n",
        "# Computing bigram probabilities using Laplace smoothing\n",
        "bigram_probs = {bigram: laplace_smoothing_bigram_prob(bigram, bigram_counts, unigram_counts, vocab_size) for bigram in bigrams}\n",
        "\n",
        "\n",
        "# Computing perplexity on the training set using the revised method\n",
        "train_perplexity = compute_perplexity_laplace(tokens, bigram_counts, unigram_counts, vocab_size)\n",
        "# Loading the development data\n",
        "with open('data/lm/dev.txt', 'r') as file:\n",
        "    dev_data = file.readlines()\n",
        "\n",
        "# Tokenizing the dev data\n",
        "dev_tokens = []\n",
        "for line in dev_data:\n",
        "    dev_tokens.extend(line.strip().split())\n",
        "\n",
        "# Replacing tokens not in the vocabulary with <UNK>\n",
        "dev_tokens = ['<UNK>' if token not in vocab else token for token in dev_tokens]\n",
        "\n",
        "# Computing perplexity on the development set\n",
        "dev_perplexity = compute_perplexity_laplace(dev_tokens, bigram_counts, unigram_counts, vocab_size)\n",
        "\n",
        "train_perplexity, dev_perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yTKPXFk8f2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "The differences between the bigram model (from Section 2.2) and the bigram model with Add-one (Laplace) smoothing:\n",
        "\n",
        "1. **Model Basics**:\n",
        "   - The bigram model from Section 2.2, without any smoothing, simply calculates the conditional probability of a word based on its preceding word using the observed frequencies in the training data.\n",
        "   - The Add-one (Laplace) smoothed bigram model modifies these probabilities by adding a constant (in this case, 1) to every bigram count to handle unseen bigrams.\n",
        "\n",
        "2. **Handling Zero Probabilities**:\n",
        "   - Without smoothing, the basic bigram model assigns a zero probability to any bigram not seen in the training data, leading to infinite perplexity for any text containing such bigrams.\n",
        "   - The Laplace smoothing method addresses this by ensuring that no bigram has a zero probability, making the model more robust to unseen or rare bigrams in the development or test data.\n",
        "\n",
        "3. **Performance**:\n",
        "   - For the basic bigram model (without smoothing), we didn't compute the exact perplexities in our earlier steps. However, it's generally expected to have higher perplexities on the development or test set due to the zero probabilities for unseen bigrams.\n",
        "   - For the Laplace smoothed bigram model, the perplexities were \\(1393.94\\) for the training set and \\(1574.41\\) for the development set. These values, while high, are finite and demonstrate the benefit of smoothing in handling unseen data.\n",
        "\n",
        "4. **Probability Distribution**:\n",
        "   - In the basic bigram model, the probability mass is distributed only among observed bigrams.\n",
        "   - In the Laplace smoothed model, some of the probability mass from observed bigrams is redistributed to unseen bigrams. This can sometimes result in overestimating the probability of rare events, especially when the vocabulary size is large.\n",
        "\n",
        "5. **Generalization**:\n",
        "   - The basic bigram model may not generalize well to unseen data due to the zero probabilities for unseen bigrams.\n",
        "   - The Laplace smoothed model, by ensuring non-zero probabilities for all bigrams, generalizes better to unseen data, but the uniform redistribution can sometimes be a naive approach, especially when more sophisticated smoothing methods (like Add-k or linear interpolation) are available.\n",
        "\n",
        "In summary, while the basic bigram model offers a direct representation of the training data, it falls short in handling unseen bigrams. The Laplace smoothed bigram model, by addressing the zero probability issue, provides a more generalizable model, albeit with potential overestimation of unseen event probabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8cFbczqlBR_"
      },
      "source": [
        "#### 2.3.2: Add-$k$ smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_ZiAgIlPUu"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.01, 134.37437799543878, 354.70541148633043)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def add_k_smoothing_bigram_prob(bigram, bigram_counts, unigram_counts, vocab_size, k):\n",
        "    \"\"\"Compute the probability of a bigram using Add-k smoothing.\"\"\"\n",
        "    return (bigram_counts[bigram] + k) / (unigram_counts[bigram[0]] + k * vocab_size)\n",
        "\n",
        "def compute_perplexity_add_k(tokens, bigram_counts, unigram_counts, vocab_size, k):\n",
        "    \"\"\"Compute the perplexity of a list of tokens using Add-k smoothed bigram probabilities.\"\"\"\n",
        "    N = len(tokens)\n",
        "    log_prob_sum = 0\n",
        "    for i in range(N - 1):\n",
        "        bigram = (tokens[i], tokens[i + 1])\n",
        "        log_prob_sum += math.log2(add_k_smoothing_bigram_prob(bigram, bigram_counts, unigram_counts, vocab_size, k))\n",
        "    return math.pow(2, -log_prob_sum / N)\n",
        "\n",
        "# Trying different k values to optimize perplexity on the dev set\n",
        "k_values = [0.01, 0.05, 0.5]\n",
        "perplexities = {}\n",
        "\n",
        "for k in k_values:\n",
        "    perplexities[k] = compute_perplexity_add_k(dev_tokens, bigram_counts, unigram_counts, vocab_size, k)\n",
        "\n",
        "best_k = min(perplexities, key=perplexities.get)\n",
        "best_dev_perplexity = perplexities[best_k]\n",
        "best_train_perplexity = compute_perplexity_add_k(tokens, bigram_counts, unigram_counts, vocab_size, best_k)\n",
        "\n",
        "best_k, best_train_perplexity, best_dev_perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHFNf8OIlQ0O"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "Comparison the bigram model with Add-one (Laplace) smoothing to the one with Add-\\( k \\) smoothing:\n",
        "\n",
        "1. **Smoothing Approach**:\n",
        "   - **Add-one (Laplace) Smoothing**: This method adds a constant value of 1 to every bigram count. This ensures non-zero probabilities for unseen bigrams but can be a blunt tool, especially with large vocabularies.\n",
        "   - **Add-\\( k \\) Smoothing**: This is a generalized version of Laplace smoothing where a fractional count \\( k \\) (which can be values like 0.01, 0.05, 0.5, etc.) is added to every bigram count. This method provides more flexibility in adjusting the amount of probability mass redistributed to unseen bigrams.\n",
        "\n",
        "2. **Performance**:\n",
        "   - **Add-one (Laplace) Smoothing**: The perplexities were \\(1393.94\\) for the training set and \\(1574.41\\) for the development set.\n",
        "   - **Add-\\( k \\) Smoothing (with best \\( k = 0.01 \\))**: The perplexities were significantly improved to \\(134.37\\) for the training set and \\(354.69\\) for the development set. This demonstrates the benefit of optimizing the smoothing parameter \\( k \\).\n",
        "\n",
        "3. **Probability Distribution**:\n",
        "   - **Add-one (Laplace) Smoothing**: This method tends to over-smooth, especially with large vocabularies. It uniformly redistributes probability mass, which can lead to overestimation of unseen bigram probabilities.\n",
        "   - **Add-\\( k \\) Smoothing**: By allowing for a fractional \\( k \\), this method can result in a more nuanced redistribution of probability mass. The optimization process helps in selecting a \\( k \\) that best fits the development data.\n",
        "\n",
        "4. **Generalization**:\n",
        "   - Both methods aim to improve generalization to unseen data by avoiding zero probabilities for unseen bigrams. However, the ability to optimize \\( k \\) in Add-\\( k \\) smoothing allows for better tuning and, as seen from the perplexities, a better fit to the development data.\n",
        "\n",
        "In conclusion, while both Add-one and Add-\\( k \\) smoothing address the challenge of zero probabilities for unseen bigrams, the Add-\\( k \\) method, with its flexibility and optimization potential, provides a more refined approach. This is evident in the improved perplexities observed for the model with Add-\\( k \\) smoothing compared to the one with Add-one smoothing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjKEO_TqlUrX"
      },
      "source": [
        "#### 2.3.3 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdd4cvYlZuO"
      },
      "source": [
        "**Code**\n",
        "\n",
        "\\# todo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([0.3, 0.5, 0.19999999999999996], 19.076434321271574, 195.06976317136474)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_trigrams(tokens):\n",
        "    \"\"\"Return a list of trigrams from the list of tokens.\"\"\"\n",
        "    return [(tokens[i], tokens[i + 1], tokens[i + 2]) for i in range(len(tokens) - 2)]\n",
        "\n",
        "def linear_interpolation_prob(token, prev_token1, prev_token2, unigram_counts, bigram_counts, trigram_counts, lambdas, total_tokens):\n",
        "    \"\"\"Compute the probability using linear interpolation.\"\"\"\n",
        "    unigram_prob = unigram_counts[token] / total_tokens\n",
        "    bigram_prob = bigram_counts.get((prev_token1, token), 0) / unigram_counts[prev_token1]\n",
        "    trigram_prob = trigram_counts.get((prev_token2, prev_token1, token), 0) / bigram_counts.get((prev_token2, prev_token1), 1)\n",
        "    \n",
        "    return lambdas[0] * unigram_prob + lambdas[1] * bigram_prob + lambdas[2] * trigram_prob\n",
        "\n",
        "def compute_perplexity_interpolation(tokens, unigram_counts, bigram_counts, trigram_counts, lambdas, total_tokens):\n",
        "    \"\"\"Compute the perplexity using linear interpolation with domain check.\"\"\"\n",
        "    N = len(tokens)\n",
        "    log_prob_sum = 0\n",
        "    for i in range(2, N):\n",
        "        prob = linear_interpolation_prob(tokens[i], tokens[i-1], tokens[i-2], unigram_counts, bigram_counts, trigram_counts, lambdas, total_tokens)\n",
        "        if prob <= 0:\n",
        "            return float('inf')  # Return infinite perplexity for invalid probabilities\n",
        "        log_prob_sum += math.log2(prob)\n",
        "    return math.pow(2, -log_prob_sum / N)\n",
        "\n",
        "# Define an optimization function to find the best lambdas\n",
        "def objective(lambdas):\n",
        "    return compute_perplexity_interpolation(dev_tokens, unigram_counts, bigram_counts, trigram_counts, lambdas, len(tokens))\n",
        "\n",
        "# Retry the optimization with the updated perplexity computation function\n",
        "best_perplexity = float('inf')\n",
        "best_lambdas = None\n",
        "\n",
        "for lambda1 in [i/10 for i in range(11)]:\n",
        "    for lambda2 in [i/10 for i in range(11 - int(lambda1*10))]:\n",
        "        lambda3 = 1 - lambda1 - lambda2\n",
        "        lambdas = [lambda1, lambda2, lambda3]\n",
        "        perplexity = objective(lambdas)\n",
        "        if perplexity < best_perplexity:\n",
        "            best_perplexity = perplexity\n",
        "            best_lambdas = lambdas\n",
        "\n",
        "# Compute the perplexity on the training and dev sets with the best lambdas\n",
        "train_perplexity_interpolation = compute_perplexity_interpolation(tokens, unigram_counts, bigram_counts, trigram_counts, best_lambdas, len(tokens))\n",
        "dev_perplexity_interpolation = best_perplexity\n",
        "\n",
        "best_lambdas, train_perplexity_interpolation, dev_perplexity_interpolation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "193.53762385240051"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the test data\n",
        "with open('./data/lm/test.txt', 'r') as file:\n",
        "    test_data = file.readlines()\n",
        "\n",
        "# Tokenizing the test data\n",
        "test_tokens = []\n",
        "for line in test_data:\n",
        "    test_tokens.extend(line.strip().split())\n",
        "\n",
        "# Replacing tokens not in the vocabulary with <UNK>\n",
        "test_tokens = ['<UNK>' if token not in vocab else token for token in test_tokens]\n",
        "\n",
        "# Computing perplexity on the test set using the best lambdas\n",
        "test_perplexity_interpolation = compute_perplexity_interpolation(test_tokens, unigram_counts, bigram_counts, trigram_counts, best_lambdas, len(tokens))\n",
        "\n",
        "test_perplexity_interpolation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyKqmQ37lcH2"
      },
      "source": [
        "**Discussion**\n",
        "\n",
        "1. **Model Mechanism**:\n",
        "   - Linear interpolation combines the strengths of unigram, bigram, and trigram models. By blending these models using optimized weights \\(\\lambda_1\\), \\(\\lambda_2\\), and \\(\\lambda_3\\), it leverages both local and broader context in predicting word probabilities.\n",
        "\n",
        "2. **Performance**:\n",
        "   - The perplexities obtained were \\(19.08\\) for the training set, \\(195.07\\) for the development set, and \\(193.54\\) for the test set.\n",
        "   - Compared to the other models discussed (bigram with and without smoothing, Add-\\(k\\) smoothing), the linear interpolation model showed the lowest (and thus best) perplexities, indicating superior predictive capability.\n",
        "\n",
        "3. **Optimized Weights**:\n",
        "   - The best \\(\\lambda\\) values that optimized perplexity on the development set were approximately:\n",
        "     - \\(\\lambda_1\\) (unigram weight): 0.3\n",
        "     - \\(\\lambda_2\\) (bigram weight): 0.5\n",
        "     - \\(\\lambda_3\\) (trigram weight): 0.2\n",
        "   - These weights show that the model relies more on the bigram context compared to unigram or trigram contexts, but still benefits from incorporating all three.\n",
        "\n",
        "4. **Handling Sparse Data**:\n",
        "   - One of the strengths of the linear interpolation approach is its robustness in handling sparse data. By blending different n-gram models, it can make predictions even when specific trigrams or bigrams are not observed in the training data.\n",
        "\n",
        "5. **Generalization**:\n",
        "   - The close perplexity values between the development set and test set indicate good generalization. The model doesn't seem to be overfitting the training data and performs well on unseen data.\n",
        "\n",
        "In summary, the linear interpolation model showcased the power of combining different n-gram models. By leveraging varied context lengths and optimizing blending weights, it achieved superior performance over the other models discussed. It represents a balanced approach that uses both local and broader contexts to predict word sequences, leading to strong results on both development and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzSbk2bClf3u"
      },
      "source": [
        "##### **Optimization**:\n",
        "\n",
        "Hyperparameter optimization is a critical step in many machine learning tasks. While we used a simple grid search in our previous steps, there are more sophisticated methods available. Here are some popular hyperparameter optimization techniques:\n",
        "\n",
        "1. **Random Search**:\n",
        "   - Instead of exhaustively searching through a predefined set of hyperparameters (like grid search), random search samples hyperparameters from a distribution over possible parameter values.\n",
        "   - This method can be more efficient than grid search, especially when the number of hyperparameters is large.\n",
        "   \n",
        "   **Example**: If you're training a neural network, you could use random search to select the learning rate from a log-uniform distribution between 0.0001 and 0.1, the dropout rate from a uniform distribution between 0 and 0.5, etc.\n",
        "\n",
        "2. **Bayesian Optimization**:\n",
        "   - This probabilistic model-based optimization technique builds a probability model of the objective function to suggest better hyperparameters.\n",
        "   - Gaussian Processes (GP) are often used as the probability model.\n",
        "   - The method balances exploration (trying new hyperparameters) and exploitation (refining current best hyperparameters).\n",
        "   \n",
        "   **Example**: If you're tuning hyperparameters for a gradient boosting machine, Bayesian optimization can suggest a combination of learning rate, number of trees, and tree depth by evaluating the regions with the highest expected improvement.\n",
        "\n",
        "3. **Gradient-based Optimization**:\n",
        "   - Some hyperparameter optimization techniques leverage gradient information to guide the search. \n",
        "   - This is particularly useful for deep learning models where certain hyperparameters can be optimized using gradient descent.\n",
        "   \n",
        "   **Example**: Using algorithms like \"Hypergradient Descent\", you can optimize hyperparameters like learning rates during the training process of a neural network.\n",
        "\n",
        "4. **Evolutionary Algorithms**:\n",
        "   - These algorithms are inspired by the process of natural selection. They involve a population of potential solutions (set of hyperparameters) which evolve over generations to improve the objective.\n",
        "   - Common techniques include genetic algorithms, particle swarm optimization, and differential evolution.\n",
        "   \n",
        "   **Example**: For a support vector machine, an evolutionary algorithm can evolve a population of solutions to find the best combination of the kernel, C parameter, and other hyperparameters.\n",
        "\n",
        "5. **Bandit-based Optimization**:\n",
        "   - This method is inspired by the multi-armed bandit problem in probability theory. It balances the exploration of new hyperparameters and exploitation of the best-known ones.\n",
        "   - Techniques like Upper Confidence Bound (UCB) or Thompson sampling can be applied.\n",
        "   \n",
        "   **Example**: When tuning hyperparameters for a random forest model, a bandit-based approach might allocate more evaluations to hyperparameter combinations that have performed well in previous iterations, while still occasionally exploring new combinations.\n",
        "\n",
        "6. **Early Stopping**:\n",
        "   - For iterative algorithms or deep learning models, training can be stopped early if the performance on a validation set stops improving. This can be considered a form of hyperparameter optimization where the number of iterations or epochs becomes the optimized hyperparameter.\n",
        "   \n",
        "   **Example**: When training a deep neural network, you can monitor the validation loss, and if it doesn't improve for a certain number of epochs (e.g., 10), you can stop the training early.\n",
        "\n",
        "In practice, the choice of hyperparameter optimization technique often depends on the nature of the problem, the model being used, and the computational resources available. It's also common to combine multiple techniques, like using Bayesian optimization with early stopping, to achieve better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgTcTlLuloHu"
      },
      "source": [
        "## 3 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jb0OQ-yltc3"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m运行具有“base”的单元格需要ipykernel包。\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/qtli/COMP7607-Fall2023/master/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://github.com/qtli/COMP7607-Fall2023/blob/master/assignments/A1/data/prep/dev.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['palestinian leader yasser arafat <PREP> wednesday welcomed the resumption <PREP> israeli-syrian peace talks , which were due to begin later <PREP> the day <PREP> the united states  .\\n',\n",
              "  'russian artillery bombarded the chechen town <PREP>  martan overnight , killing two children and wounding two people  .\\n',\n",
              "  'million units sold , the national association <PREP> realtors reported wednesday  .\\n',\n",
              "  'some , homes <PREP> the north <PREP> scotland were left without electricity wednesday under exceptionally poor weather conditions , with snow falling <PREP> the fifth consecutive day , the scottish electricity company hydro electric announced  .\\n',\n",
              "  'german automaker mercedes-benz , a member <PREP> daimler-benz group , will report a  percent increase <PREP> sales <PREP> this year , to around  billion marks -lrb-  billion dollars -rrb- , group chairman helmut werner said <PREP> wednesday  .\\n'],\n",
              " ['on of in in\\n', 'of\\n', 'of\\n', 'in of for\\n', 'of in for on\\n'])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the contents of dev.in and dev.out files\n",
        "with open(\"./data/prep/dev.in\", \"r\") as file:\n",
        "    dev_in_content = file.readlines()\n",
        "\n",
        "with open(\"./data/prep/dev.out\", \"r\") as file:\n",
        "    dev_out_content = file.readlines()\n",
        "\n",
        "# Display the first few lines of each file for review\n",
        "dev_in_content[:5], dev_out_content[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['palestinian leader yasser arafat on wednesday welcomed the resumption of israeli-syrian peace talks , which were due to begin later in the day in the united states  .',\n",
              " 'russian artillery bombarded the chechen town of  martan overnight , killing two children and wounding two people  .',\n",
              " 'million units sold , the national association of realtors reported wednesday  .',\n",
              " 'some , homes in the north of scotland were left without electricity wednesday under exceptionally poor weather conditions , with snow falling for the fifth consecutive day , the scottish electricity company hydro electric announced  .',\n",
              " 'german automaker mercedes-benz , a member of daimler-benz group , will report a  percent increase in sales for this year , to around  billion marks -lrb-  billion dollars -rrb- , group chairman helmut werner said on wednesday  .']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract sentences and their corresponding prepositions from dev.in and dev.out\n",
        "\n",
        "# Split sentences and replace <PREP> tokens with placeholders for formatting\n",
        "sentences = [sentence.replace(\"<PREP>\", \"{}\").strip() for sentence in dev_in_content]\n",
        "\n",
        "# Extract prepositions from dev.out\n",
        "prepositions = [preps.strip().split() for preps in dev_out_content]\n",
        "\n",
        "# Format the sentences by filling in the placeholders with the correct prepositions\n",
        "formatted_sentences = [sentence.format(*preps) for sentence, preps in zip(sentences, prepositions)]\n",
        "\n",
        "# Display the first few formatted sentences for review\n",
        "formatted_sentences[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('palestinian', 'leader'): Counter({'yasser': 0.875, 'on': 0.125}),\n",
              " ('leader', 'yasser'): Counter({'arafat': 1.0}),\n",
              " ('yasser',\n",
              "  'arafat'): Counter({'on': 0.15384615384615385,\n",
              "          'will': 0.15384615384615385,\n",
              "          'announced': 0.07692307692307693,\n",
              "          'until': 0.07692307692307693,\n",
              "          'was': 0.07692307692307693,\n",
              "          'declared': 0.07692307692307693,\n",
              "          'chaired': 0.07692307692307693,\n",
              "          'after': 0.07692307692307693,\n",
              "          'called': 0.07692307692307693,\n",
              "          'saturday': 0.07692307692307693,\n",
              "          'to': 0.07692307692307693}),\n",
              " ('arafat', 'on'): Counter({'wednesday': 1.0}),\n",
              " ('on',\n",
              "  'wednesday'): Counter({'.': 0.21428571428571427,\n",
              "          ',': 0.14285714285714285,\n",
              "          'to': 0.07142857142857142,\n",
              "          'night': 0.07142857142857142,\n",
              "          'met': 0.07142857142857142,\n",
              "          'welcomed': 0.03571428571428571,\n",
              "          'held': 0.03571428571428571,\n",
              "          'following': 0.03571428571428571,\n",
              "          'as': 0.03571428571428571,\n",
              "          'called': 0.03571428571428571,\n",
              "          'on': 0.03571428571428571,\n",
              "          'hailed': 0.03571428571428571,\n",
              "          'tabled': 0.03571428571428571,\n",
              "          'inaugurated': 0.03571428571428571,\n",
              "          'signed': 0.03571428571428571,\n",
              "          'condemned': 0.03571428571428571,\n",
              "          'for': 0.03571428571428571})}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def build_ngram_model(sentences, n=3):\n",
        "    \"\"\"\n",
        "    Build an n-gram model from the given sentences.\n",
        "    \n",
        "    Parameters:\n",
        "    - sentences: List of sentences to build the model from.\n",
        "    - n: The size of the n-gram.\n",
        "    \n",
        "    Returns:\n",
        "    - A dictionary representing the n-gram model.\n",
        "    \"\"\"\n",
        "    ngrams = defaultdict(Counter)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        for i in range(len(words) - n + 1):\n",
        "            prefix = tuple(words[i:i+n-1])\n",
        "            suffix = words[i+n-1]\n",
        "            ngrams[prefix][suffix] += 1\n",
        "\n",
        "    # Normalize the counts to get probabilities\n",
        "    for prefix, counter in ngrams.items():\n",
        "        total = sum(counter.values())\n",
        "        for suffix in counter:\n",
        "            counter[suffix] /= total\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "# Build a trigram model (n=3) using the training data\n",
        "ngram_model = build_ngram_model(formatted_sentences, n=3)\n",
        "\n",
        "# Display some entries of the n-gram model for review\n",
        "dict(list(ngram_model.items())[:5])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
